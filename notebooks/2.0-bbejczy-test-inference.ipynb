{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load modules and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnbj/miniconda3/envs/multitask-mayhem/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import src.utils as utils\n",
    "from src.data.dataloader import mtlDataModule \n",
    "import random\n",
    "\n",
    "from src.data.manifests import generate_manifest\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from src.visualization.draw_things import draw_bounding_boxes\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import load_yaml\n",
    "from src.models.model_loader import ModelLoader\n",
    "import torch\n",
    "\n",
    "ROOT_DIR = \"/home/bnbj/repos/multitask-mayhem\"\n",
    "\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "CLASS_LOOKUP = load_yaml(ROOT_DIR+\"/configs/class_lookup.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get config to initialize `pl.DataModule` and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "config_file = ROOT_DIR + \"/models/fasterrcnn_mobilenetv3_baseline_22-12-04T190124/fasterrcnn_mobilenetv3_baseline_22-12-04T190124.yaml\"\n",
    "\n",
    "\n",
    "data_module = mtlDataModule(config_path=config_file)\n",
    "\n",
    "seg, det = False, False\n",
    "if data_module.config[\"model\"] in [\"fasterrcnn\", \"fasterrcnn_mobilenetv3\", \"ssdlite\"]:\n",
    "    det = True\n",
    "elif data_module.config[\"model\"] in [\"deeplabv3\"]:\n",
    "    seg = True\n",
    "\n",
    "data_module.config[\"batch_size\"] = 1\n",
    "data_module.config[\"num_workers\"] = 0\n",
    "data_module.config[\"shuffle\"] = False\n",
    "\n",
    "model_loader = ModelLoader(config=data_module.config)\n",
    "model = model_loader.grab_model()\n",
    "model_folder = str(Path(config_file).parents[0])\n",
    "model.load_state_dict(torch.load(model_folder+\"/weights/best.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup(stage=\"fit\")\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "data_module.setup(stage=\"validate\")\n",
    "valid_dataloader = data_module.val_dataloader()\n",
    "data_module.setup(stage=\"test\")\n",
    "test_dataloader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare landing folder and label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference = ROOT_DIR+\"/notebooks/test_inference\"\n",
    "\n",
    "if os.path.exists(test_inference):\n",
    "    shutil.rmtree(test_inference)\n",
    "\n",
    "os.makedirs(test_inference, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        test_set = utils.list_files_with_extension(\"data/test/2022-09-23-10-07-37/synchronized_l515_image/\", \".png\", \"path\")\n",
    "        random.seed(42)\n",
    "        self.image_list = random.sample(test_set, 200)\n",
    "        self.transforms = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.transforms(Image.open(self.image_list[idx]))\n",
    "        return image.type(torch.FloatTensor)\n",
    "\n",
    "img_dataset = ImageDataset()\n",
    "\n",
    "test_set_dataloader = DataLoader(img_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_of_tensors_to_tensor(tuple_of_tensors):\n",
    "    return torch.stack(list(tuple_of_tensors), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnbj/repos/multitask-mayhem/src/visualization/draw_things.py:68: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    image, target = batch\n",
    "\n",
    "    image = tuple_of_tensors_to_tensor(image)\n",
    "\n",
    "    preds = model(image)\n",
    "    \n",
    "    if det:\n",
    "        preds = preds[0]\n",
    "        boxes = preds[\"boxes\"]\n",
    "        labels = preds[\"labels\"]\n",
    "        scores = preds[\"scores\"]\n",
    "        score_mask = scores > 0.1\n",
    "\n",
    "\n",
    "        boxes_filtered = boxes[score_mask]\n",
    "        labels_filtered = labels[score_mask]\n",
    "        scores_filtered = scores[score_mask]\n",
    "    \n",
    "\n",
    "\n",
    "        label_names = [CLASS_LOOKUP[\"bbox_rev\"][label.item()] for label in labels_filtered]\n",
    "    elif seg:\n",
    "        masks = preds[\"out\"]\n",
    "        masks = torch.sigmoid(masks)\n",
    "        masks = (masks>0.5)\n",
    "\n",
    "    \n",
    "    img = image.mul(255).type(torch.uint8)\n",
    "\n",
    "    if det:\n",
    "        drawn_image = draw_bounding_boxes(\n",
    "            image = img.squeeze(0),\n",
    "            boxes = boxes_filtered,\n",
    "            labels = label_names,\n",
    "            scores = scores_filtered\n",
    "            )\n",
    "    elif seg:\n",
    "        drawn_image = draw_segmentation_masks(img.squeeze(0), masks.squeeze(0), alpha=0.5, colors=\"green\")\n",
    "    \n",
    "    image_pil = T.ToPILImage()(drawn_image)\n",
    "    image_pil.save(test_inference+\"/{}.png\".format(i))\n",
    "    \n",
    "    # image_pil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f', 5), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "list1 = [\"a\", \"b\", \"c\", \"d\", \"f\"]\n",
    "list2 = [1,2,3,4,5]\n",
    "\n",
    "list_zip = list(zip(list1, list2))\n",
    "\n",
    "list_random = random.sample(list_zip, 3)\n",
    "list_random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb71c2371d005dd775ea25e722093bd9e2e673fc6da8da55d0d718129c6a4c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

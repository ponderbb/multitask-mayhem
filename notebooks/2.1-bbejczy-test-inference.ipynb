{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load modules and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnbj/miniconda3/envs/multitask-mayhem/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import src.utils as utils\n",
    "from src.data.dataloader import mtlDataModule \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.manifests import generate_manifest\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from src.visualization.draw_things import draw_bounding_boxes\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import load_yaml\n",
    "from src.models.model_loader import ModelLoader\n",
    "import torch\n",
    "\n",
    "ROOT_DIR = \"/home/bnbj/repos/multitask-mayhem\"\n",
    "\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "CLASS_LOOKUP = load_yaml(ROOT_DIR+\"/configs/class_lookup.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get config to initialize `pl.DataModule` and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "config_file = ROOT_DIR + \"/models/fasterrcnn_mobilenetv3_baseline_22-12-04T190124/fasterrcnn_mobilenetv3_baseline_22-12-04T190124.yaml\"\n",
    "\n",
    "\n",
    "data_module = mtlDataModule(config_path=config_file)\n",
    "\n",
    "seg, det = False, False\n",
    "if data_module.config[\"model\"] in [\"fasterrcnn\", \"fasterrcnn_mobilenetv3\", \"ssdlite\"]:\n",
    "    det = True\n",
    "elif data_module.config[\"model\"] in [\"deeplabv3\"]:\n",
    "    seg = True\n",
    "\n",
    "data_module.config[\"batch_size\"] = 1\n",
    "data_module.config[\"num_workers\"] = 0\n",
    "data_module.config[\"shuffle\"] = False\n",
    "\n",
    "model_loader = ModelLoader(config=data_module.config)\n",
    "model = model_loader.grab_model()\n",
    "model_folder = str(Path(config_file).parents[0])\n",
    "model.load_state_dict(torch.load(model_folder+\"/weights/best.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup(stage=\"fit\")\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "data_module.setup(stage=\"validate\")\n",
    "valid_dataloader = data_module.val_dataloader()\n",
    "data_module.setup(stage=\"test\")\n",
    "test_dataloader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare landing folder and label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference = ROOT_DIR+\"/notebooks/test_inference\"\n",
    "test_set = \"data/test/2022-09-23-10-07-37/synchronized_l515_image/\"\n",
    "\n",
    "if os.path.exists(test_inference):\n",
    "    shutil.rmtree(test_inference)\n",
    "os.makedirs(test_inference, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, test_set, downsample:bool=False) -> None:\n",
    "        super().__init__()\n",
    "        test_set = utils.list_files_with_extension(test_set, \".png\", \"path\")\n",
    "        random.seed(42)\n",
    "        if downsample:\n",
    "            self.image_list = random.sample(test_set, 200)\n",
    "        else:\n",
    "            self.image_list = test_set\n",
    "        self.transforms = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.transforms(Image.open(self.image_list[idx]))\n",
    "        return image.type(torch.FloatTensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = ImageDataset(test_set, downsample=True)\n",
    "\n",
    "test_set_dataloader = DataLoader(img_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_of_tensors_to_tensor(tuple_of_tensors):\n",
    "    return torch.stack(list(tuple_of_tensors), dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on single model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnbj/repos/multitask-mayhem/src/visualization/draw_things.py:68: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    image, target = batch\n",
    "\n",
    "    image = tuple_of_tensors_to_tensor(image)\n",
    "\n",
    "    preds = model(image)\n",
    "    \n",
    "    if det:\n",
    "        preds = preds[0]\n",
    "        boxes = preds[\"boxes\"]\n",
    "        labels = preds[\"labels\"]\n",
    "        scores = preds[\"scores\"]\n",
    "        score_mask = scores > 0.5\n",
    "\n",
    "\n",
    "        boxes_filtered = boxes[score_mask]\n",
    "        labels_filtered = labels[score_mask]\n",
    "        scores_filtered = scores[score_mask]\n",
    "    \n",
    "\n",
    "\n",
    "        label_names = [CLASS_LOOKUP[\"bbox_rev\"][label.item()] for label in labels_filtered]\n",
    "    elif seg:\n",
    "        masks = preds[\"out\"]\n",
    "        masks = torch.sigmoid(masks)\n",
    "        masks = (masks>0.5)\n",
    "\n",
    "    \n",
    "    img = image.mul(255).type(torch.uint8)\n",
    "\n",
    "    if det:\n",
    "        drawn_image = draw_bounding_boxes(\n",
    "            image = img.squeeze(0),\n",
    "            boxes = boxes_filtered,\n",
    "            labels = label_names,\n",
    "            scores = scores_filtered\n",
    "            )\n",
    "    elif seg:\n",
    "        drawn_image = draw_segmentation_masks(img.squeeze(0), masks.squeeze(0), alpha=0.5, colors=\"green\")\n",
    "    \n",
    "    image_pil = T.ToPILImage()(drawn_image)\n",
    "    image_pil.save(test_inference+\"/{}.png\".format(i))\n",
    "    \n",
    "    # image_pil.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on ensemble"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_seg = ROOT_DIR + \"/models/baseline-zoo/deeplabv3_baselinev3_22-12-08T144830/deeplabv3_baselinev3_22-12-08T144830.yaml\"\n",
    "config_det = ROOT_DIR + \"/models/baseline-zoo/fasterrcnn_mobilenetv3_baselinev3_22-12-08T154854/fasterrcnn_mobilenetv3_baselinev3_22-12-08T154854.yaml\"\n",
    "\n",
    "demo_set = \"/home/bnbj/repos/multitask-mayhem/data/test/demo-wo-duplicates/test/synchronized_l515_image/\"\n",
    "demo_inf = ROOT_DIR+\"/notebooks/demo_inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_module = mtlDataModule(config_path=config_det)\n",
    "config = utils.load_yaml(config_det)\n",
    "\n",
    "# load detection model\n",
    "model_det = ModelLoader.grab_model(config=config)\n",
    "model_folder = str(Path(config_det).parents[0])\n",
    "model_det.load_state_dict(torch.load(model_folder+\"/weights/best.pth\", map_location=device))\n",
    "\n",
    "\n",
    "# load segmentation model\n",
    "config[\"model\"] = \"deeplabv3\"\n",
    "model_seg = ModelLoader.grab_model(config=config)\n",
    "model_folder = str(Path(config_seg).parents[0])\n",
    "model_seg.load_state_dict(torch.load(model_folder+\"/weights/best.pth\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6389"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_dataset = ImageDataset(demo_set, downsample=False)\n",
    "demo_dataloader = DataLoader(demo_dataset, shuffle=False, batch_size=1, num_workers=0)\n",
    "len(demo_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(demo_inf):\n",
    "    shutil.rmtree(demo_inf)\n",
    "os.makedirs(demo_inf, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:31,  1.37it/s]/home/bnbj/repos/multitask-mayhem/src/visualization/draw_things.py:68: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n",
      "6389it [1:18:23,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model_det.eval()\n",
    "model_seg.eval()\n",
    "\n",
    "for i, batch in tqdm(enumerate(demo_dataloader)):\n",
    "\n",
    "    image = tuple_of_tensors_to_tensor(batch)\n",
    "\n",
    "    ## detection\n",
    "    preds = model_det(image)\n",
    "    \n",
    "    preds = preds[0]\n",
    "    boxes = preds[\"boxes\"]\n",
    "    labels = preds[\"labels\"]\n",
    "    scores = preds[\"scores\"]\n",
    "    score_mask = scores > 0.5\n",
    "\n",
    "    boxes_filtered = boxes[score_mask]\n",
    "    labels_filtered = labels[score_mask]\n",
    "    scores_filtered = scores[score_mask]\n",
    "\n",
    "    label_names = [CLASS_LOOKUP[\"bbox_rev\"][label.item()] for label in labels_filtered]\n",
    "\n",
    "    ## segmentation\n",
    "    preds_seg = model_seg(image)\n",
    "\n",
    "    masks = preds_seg[\"out\"]\n",
    "    masks = torch.sigmoid(masks)\n",
    "    masks = (masks>0.5)\n",
    "\n",
    "    \n",
    "    img = image.mul(255).type(torch.uint8)\n",
    "\n",
    "    drawn_image_det = draw_bounding_boxes(\n",
    "        image = img.squeeze(0),\n",
    "        boxes = boxes_filtered,\n",
    "        labels = label_names,\n",
    "        scores = scores_filtered\n",
    "        )\n",
    "    \n",
    "    drawn_image_seg = draw_segmentation_masks(drawn_image_det, masks.squeeze(0), alpha=0.5, colors=\"green\")\n",
    "    \n",
    "    image_pil = T.ToPILImage()(drawn_image_seg)\n",
    "    image_pil.save(demo_inf+\"/{}.png\".format(str(i).zfill(6)))\n",
    "    \n",
    "    # image_pil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb71c2371d005dd775ea25e722093bd9e2e673fc6da8da55d0d718129c6a4c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
